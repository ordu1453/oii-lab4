
\chapter{Практический раздел}

\section{Программная реализация}

Программная реализация выполнялась на языке Python с использованием библиотеки PyTorch.  

\section{Результаты экспериментов}

При увеличении доли обучающей выборки точность модели растёт.  
Сети с большим количеством скрытых слоёв демонстрируют более высокую точность на train выборке, что может приводить к переобучению.  

Для оценки минимального размера выборки использовалось неравенство Чебышёва, что позволило определить необходимое количество данных для гарантированного достижения заданной точности.

% \begin{figure}[H]
% \centering
% \includegraphics[width=0.8\textwidth]{accuracy_graph.png}
% \caption{Train и Test accuracy для моделей Net0, Net1 и Net5 в зависимости от доли обучающей выборки}
% \end{figure}

\chapter*{ВЫВОДЫ}

\begin{enumerate}
    \item Архитектура сети и количество скрытых слоёв напрямую влияют на точность классификации и риск переобучения.
    \item Увеличение доли обучающей выборки повышает обобщающую способность модели.
    \item Минимальный размер выборки для гарантированного качества можно рассчитать с помощью неравенства Чебышёва.
    \item На практике для MNIST сеть с одним или несколькими скрытыми слоями и ReLU активацией достигает высокой точности даже на ограниченной выборке.
\end{enumerate}



% \includelisting
%     {gen.py} % Имя файла с расширением (файл должен быть расположен в директории inc/lst/)
%     {Python} % Язык программирования (необязательный аргумент)
%     {Инициализация решения с применением генетического алгоритма} % Подпись листинга

