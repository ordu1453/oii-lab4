
\chapter{Практический раздел}

\section{Программная реализация}

Программная реализация выполнялась на языке Python с использованием библиотеки PyTorch.

Для создания нейронной сети необходимо задать класс, внутри которого прописаны каждый из слоев сети, как представлено в листинге \ref{lst:net.py}.

\includelistingpretty
    {net.py} % Имя файла с расширением (файл должен быть расположен в директории inc/lst/)
    {python} % Язык программирования (необязательный аргумент)
    {Создание класса нейронной сети с одним скрытым слоем} % Подпись листинга

Для создания нейронной сети с пятью скрытыми слоями аналогично создается отдельный класс, как представлено на рисунке \ref{lst:net5.py}.

\includelistingpretty
    {net5.py} % Имя файла с расширением (файл должен быть расположен в директории inc/lst/)
    {python} % Язык программирования (необязательный аргумент)
    {Создание класса нейронной сети с пятью скрытыми слоями} % Подпись листинга

Для задания KL Divergence в качестве функции потерь, использовался встроенный в библиотеку PyTorch класс KLDivLoss, как представлено на рисунке \ref{lst:kl.py}.

\includelistingpretty
    {kl.py} % Имя файла с расширением (файл должен быть расположен в директории inc/lst/)
    {python} % Язык программирования (необязательный аргумент)
    {Задание KL Divergence в качестве функции потерь} % Подпись листинга

\section{Результаты}

Количество эпох обучения моделей нейронный сетей было выбрано равным пяти. Зависимость точности обученной модели нейронной сети от доли обучающей выборки представлен на рисунке \ref{img:ep5_final}. 

Как видно из рисунка, при увеличении доли обучающей выборки точность модели растёт.  
Сети с большим количеством скрытых слоёв демонстрируют более высокую точность на обоих выборках. Причем разница между моделью без скрытых слоев и моделями с хотя бы одним скрытым слоем более существенная, чем разница между моделями с одним и пятью скрытыми слоями - разница между ними на тестовой выборке получилось несущественной.

Для оценки того, какое минимальное количество обучающих примеров необходимо для гарантированного достижения заданной точности, было использовано неравенство Чебышёва, приведенное в отчете ранее.

В рамках эксперимента были использованы значения для модели с наибольшим процентом точности на тестовой выборке (5 скрытых слоев, 80$\%$ обучающей выборки):
\begin{itemize}
    \item точность обученной модели на тестовой выборке: $p = 0.9738$,
    \item допустимое отклонение: $\epsilon = 0.05$,
    \item требуемая вероятность достижения точности: $P = 0.95$, следовательно $\delta = 0.05$.
\end{itemize}

Подставляя значения, получаем:

\begin{equation}
N \ge \frac{0.9738 \cdot (1 - 0.9738)}{0.05 \cdot 0.05^2}
     \approx 204
\end{equation}

Таким образом, для гарантии того, что средняя точность модели отклонится от истинной не более чем на $\epsilon = 0.05$ с вероятностью $P=0.95$, достаточно иметь около:

\begin{equation}
N \approx 204 \text{ обучающих примера}
\end{equation}


Полученное значение означает, что даже относительно небольшая обучающая выборка с высокой вероятностью позволяет получить среднюю точность, отличающуюся от истинной не более чем на 5\%.


\includeimage
    {ep5_final} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
    {f} % Обтекание (без обтекания)
    {h} % Положение рисунка (см. figure из пакета float)
    {1\textwidth} % Ширина рисунка
    {Зависимость точности моделей от доли обучающей выборки} % Подпись рисунка



\chapter*{ВЫВОДЫ}

\begin{enumerate}
    \item Архитектура сети и количество скрытых слоёв напрямую влияют на точность классификации и риск переобучения.
    \item Увеличение доли обучающей выборки повышает точность модели.
    \item Полученное по неравенству Чебышёва значение минимального количества обучающих выборок на порядки меньше количества выборок в базе данных MNIST (около 60000). Переобучение модели практически невозможно.
\end{enumerate}



% \includelisting
%     {gen.py} % Имя файла с расширением (файл должен быть расположен в директории inc/lst/)
%     {Python} % Язык программирования (необязательный аргумент)
%     {Инициализация решения с применением генетического алгоритма} % Подпись листинга

